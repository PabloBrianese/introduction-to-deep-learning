{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b3c3c-8b0c-48c5-ad80-242d37dba3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f53e7-a3a1-4f44-a875-3cdee0d5a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "pixels = np.load('digits_pixels.npy')\n",
    "n_cols = pixels.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "numbers = to_categorical(np.load('digits_target.npy'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pixels_train, pixels_test, numbers_train, numbers_test = \\\n",
    "  train_test_split(pixels, numbers, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a478e-8000-4e4a-96b8-8afa26f20efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(16, activation='relu', input_shape=input_shape),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211dbc7-34c7-4408-a9c5-4ecaf5265f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the untrained model works as expected\n",
    "model.predict(pixels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c4615-c371-49cd-9592-d4a871117ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "history = model.fit(pixels_train, numbers_train, epochs=60, validation_data=(pixels_test, numbers_test), verbose=False)\n",
    "\n",
    "plt.plot(history.history['loss'], color='black', label='Training loss')\n",
    "plt.plot(history.history['val_loss'], color='red', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('categorical_crossentropy')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1a789-9622-4ec1-a352-a34787641fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the effect of adding more training data?\n",
    "# Create an untrained model to use for experimentation\n",
    "\n",
    "# Untrained model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(16, activation='relu', input_shape=input_shape),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save initial weights to allow for experimentation\n",
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5826aa7-bee3-4484-bb02-acff5558b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=1)\n",
    "\n",
    "# Experiment metrics\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Define the number of training sizes\n",
    "# to use in each round of experimentation\n",
    "training_sizes = np.array([ 125,  502,  879, 1255])\n",
    "for training_size in training_sizes:\n",
    "  # Get a fraction of the training data\n",
    "  pixels_train_frac = pixels_train[:training_size]\n",
    "  numbers_train_frac = numbers_train[:training_size]\n",
    "  \n",
    "  # Reset the model to the initial weights\n",
    "  # And train it on the new data fraction\n",
    "  model.set_weights(initial_weights)\n",
    "  model.fit(pixels_train_frac, numbers_train_frac, epochs=50, callbacks=[early_stop], verbose=False)\n",
    "\n",
    "  # Record results\n",
    "  train_accuracy = model.evaluate(pixels_train_frac, numbers_train_frac)[1]\n",
    "  test_accuracy = model.evaluate(pixels_test, numbers_test)[1]\n",
    "  \n",
    "  train_accuracies.append(train_accuracy)\n",
    "  test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37d1ff-4807-4621-a05e-2b1c81be233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, color='black', label='Training accuracy')\n",
    "plt.plot(test_accuracies, color='red', label='Validation accuracy')\n",
    "plt.xlabel('Dataset size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854fa7b-7e30-4a6e-85ba-d8bc5e65659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We go back to the irrigation model\n",
    "\n",
    "# Parcel irrigation data\n",
    "# Three parcels, Â¿do we have to irrigate or not?\n",
    "irrigation = pd.read_csv('irrigation_machine.csv').drop('Unnamed: 0', axis=1)\n",
    "parcel_columns = ['parcel_0', 'parcel_1', 'parcel_2']\n",
    "parcels = irrigation[parcel_columns].values\n",
    "sensors = irrigation.drop(parcel_columns, axis=1).values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sensors_train, sensors_test, parcels_train, parcels_test = \\\n",
    "  train_test_split(sensors, parcels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88e808-0c99-4fa1-af18-0965ed88d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We produce new models using a function\n",
    "# This allow us to quickly experiment with different activation functions\n",
    "\n",
    "def get_model(activation_function):\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "  nodes = 2**7\n",
    "  rate = 0.8\n",
    "\n",
    "  model = Sequential([\n",
    "    Dense(nodes, activation=activation_function, input_shape=(20,)),\n",
    "    Dropout(rate=rate),\n",
    "    Dense(nodes, activation=activation_function),\n",
    "    Dropout(rate=rate),\n",
    "    Dense(nodes, activation=activation_function),\n",
    "    Dropout(rate=rate),\n",
    "    Dense(nodes, activation=activation_function),\n",
    "    Dropout(rate=rate),\n",
    "    Dense(nodes, activation=activation_function),\n",
    "    Dropout(rate=rate),\n",
    "    Dense(3, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  return model\n",
    "\n",
    "# Example\n",
    "model = get_model('relu')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460d328-e3de-48be-a7a5-69b2992b1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Store experimentation results\n",
    "activation_results = {}\n",
    "\n",
    "for activation in activations:\n",
    "  model = get_model(activation)\n",
    "  history = model.fit(sensors_train, parcels_train, epochs=2**10, validation_data=(sensors_test, parcels_test), verbose=False)\n",
    "  activation_results[activation] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0480e85-9d7f-42f1-9718-3d906e3af8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_df(metric):\n",
    "  return pd.DataFrame({activation: activation_results[activation].history[metric] for activation in activations})\n",
    "\n",
    "loss = metric_df('loss')\n",
    "val_loss = metric_df('val_loss')\n",
    "accuracy = metric_df('accuracy')\n",
    "val_accuracy = metric_df('val_accuracy')\n",
    "\n",
    "\n",
    "loss.plot()\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"binary_crossentropy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "val_loss.plot()\n",
    "plt.title(\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"binary_crossentropy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "accuracy.plot()\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()\n",
    "\n",
    "val_accuracy.plot()\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()\n",
    "\n",
    "sp.special.logit(accuracy).plot()\n",
    "plt.title(\"Training logit accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"logit(accuracy)\")\n",
    "plt.show()\n",
    "\n",
    "sp.special.logit(val_accuracy).plot()\n",
    "plt.title(\"Validation logit accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"logit(accuracy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6deb9-9245-4f3a-a798-e6f7ff8ac14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic classification problem using scikit-learn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(n_samples=1000, noise=.1, factor=.3)\n",
    "n_cols = X.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.3, stratify=y)\n",
    "\n",
    "n_training_samples = X_train.shape[0]\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0243faa-bd66-40c8-8f99-a2e438f1e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring batch sizes\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(12, activation='relu', input_shape=input_shape),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train using different batch sizes: 1, ..., n_training_samples\n",
    "# Be amazed\n",
    "batch_size = 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=batch_size)\n",
    "\n",
    "testing_metrics = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"The accuracy when using a batch of size 1 is: {testing_metrics[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8752b-a540-4edc-8af0-0814a733cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisit the MNIST dataset\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "pixels = np.load('digits_pixels.npy')\n",
    "n_cols = pixels.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "numbers = to_categorical(np.load('digits_target.npy'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pixels_train, pixels_test, numbers_train, numbers_test = \\\n",
    "  train_test_split(pixels, numbers, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce8278-2e9b-4f4a-86be-ce6a15ca5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization in the MNIST digits dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "standard_model = Sequential([\n",
    "  Dense(50, activation='relu', input_shape=input_shape, kernel_initializer='normal'),\n",
    "  Dense(50, activation='relu', kernel_initializer='normal'),\n",
    "  Dense(50, activation='relu', kernel_initializer='normal'),\n",
    "  Dense(10, activation='softmax', kernel_initializer='normal')\n",
    "])\n",
    "standard_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# BatchNormalization --> kernel_initializer='normal'\n",
    "\n",
    "batchnorm_model = Sequential([\n",
    "  Dense(50, activation='relu', input_shape=input_shape, kernel_initializer='normal'),\n",
    "  BatchNormalization(),\n",
    "  Dense(50, activation='relu', kernel_initializer='normal'),\n",
    "  BatchNormalization(),\n",
    "  Dense(50, activation='relu', kernel_initializer='normal'),\n",
    "  BatchNormalization(),\n",
    "  Dense(10, activation='softmax', kernel_initializer='normal')\n",
    "])\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(standard_model.summary())\n",
    "print(batchnorm_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113659d-3143-4131-8f72-7705cdd67dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "training_data = (pixels_train, numbers_train)\n",
    "validation_data = (pixels_test, numbers_test)\n",
    "epochs = 2**6\n",
    "history['standard'] = standard_model.fit(*training_data, validation_data=validation_data, epochs=epochs, verbose=False)\n",
    "history['batchnorm'] = batchnorm_model.fit(*training_data, validation_data=validation_data, epochs=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b0a22-04d1-473e-b13d-5d34c68b2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['standard'].history['loss'], label='standard_model training loss', color='blue', linestyle='dashed')\n",
    "plt.plot(history['standard'].history['val_loss'], label='standard_model validation loss', color='blue', linestyle='solid')\n",
    "plt.plot(history['batchnorm'].history['loss'], label='batchnorm_model training loss', color='red', linestyle='dashed')\n",
    "plt.plot(history['batchnorm'].history['val_loss'], label='batchnorm_model validation loss', color='red', linestyle='solid')\n",
    "plt.title('Batch normalization effects over Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('categorical_crossentropy')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298589c-6663-4e35-9626-04d30ebb0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['standard'].history['accuracy'], label='standard_model training accuracy', color='blue', linestyle='dashed')\n",
    "plt.plot(history['standard'].history['val_accuracy'], label='standard_model validation accuracy', color='blue', linestyle='solid')\n",
    "plt.plot(history['batchnorm'].history['accuracy'], label='batchnorm_model training accuracy', color='red', linestyle='dashed')\n",
    "plt.plot(history['batchnorm'].history['val_accuracy'], label='batchnorm_model validation accuracy', color='red', linestyle='solid')\n",
    "plt.title('Batch normalization effects over Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4dcb8-de82-4f5f-b0d3-100374d6eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "plt.plot(logit(history['standard'].history['accuracy']), label='standard_model training accuracy', color='blue', linestyle='dashed')\n",
    "plt.plot(logit(history['standard'].history['val_accuracy']), label='standard_model validation accuracy', color='blue', linestyle='solid')\n",
    "plt.plot(logit(history['batchnorm'].history['accuracy']), label='batchnorm_model training accuracy', color='red', linestyle='dashed')\n",
    "plt.plot(logit(history['batchnorm'].history['val_accuracy']), label='batchnorm_model validation accuracy', color='red', linestyle='solid')\n",
    "plt.title('Batch normalization effects over Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('logit(accuracy)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d39669-7d80-4358-884d-870934311dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "breast_cancer_data = pd.read_csv('breast_cancer.csv').drop(['id', 'Unnamed: 32'], axis=1)\n",
    "\n",
    "cell_characteristics = breast_cancer_data.drop('diagnosis', axis=1).values\n",
    "\n",
    "breast_cancer_data['diagnosis'] = breast_cancer_data['diagnosis'].astype('category').cat.codes\n",
    "diagnosis = breast_cancer_data['diagnosis'].values\n",
    "\n",
    "cell_characteristics_train, cell_characteristics_test, diagnosis_train, diagnosis_test = \\\n",
    "  train_test_split(cell_characteristics, diagnosis, train_size=.3, stratify=diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40228e98-2bce-48b0-8c01-299efaffc70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fb1b848e400>,\n",
       "                   param_distributions={'activation': ['relu', 'tanh'],\n",
       "                                        'batch_size': [32, 128, 256],\n",
       "                                        'epochs': [50, 100, 200],\n",
       "                                        'learning_rate': [0.1, 0.01, 0.001]})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameter tuning\n",
    "\n",
    "# Model factory\n",
    "def create_model(learning_rate, activation):\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import Dense\n",
    "  from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "  model = Sequential([\n",
    "    Dense(128, activation=activation, input_shape=(30,)),\n",
    "    Dense(256, activation=activation),\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  optimizer = Adam(learning_rate=learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  return model\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Use model factory to create a classification model\n",
    "# that mimics the scikit-learn API!!!\n",
    "model = KerasClassifier(build_fn=create_model, verbose=False)\n",
    "\n",
    "# Now we can use scikit-learn to perform hyper-parameter optimization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "  'activation': ['relu', 'tanh'],\n",
    "  'batch_size': [2**5, 2**7, 2**8],\n",
    "  'epochs': [50, 100, 200],\n",
    "  'learning_rate': [.1, .01, .001]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_distributions=params, cv=3)\n",
    "\n",
    "search.fit(cell_characteristics_train, diagnosis_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aed109c5-0a9e-44f3-ade8-2f40db9d24d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>256</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.923872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.917920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.917920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.906328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.900167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.882728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>128</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.724937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.629699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.629699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.518588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_activation param_batch_size param_epochs param_learning_rate  \\\n",
       "9             relu              256          200                0.01   \n",
       "0             relu              256           50                0.01   \n",
       "8             tanh               32          200               0.001   \n",
       "1             tanh               32           50               0.001   \n",
       "6             tanh              128          100                0.01   \n",
       "3             tanh              256          100                0.01   \n",
       "7             relu              128          200                 0.1   \n",
       "2             tanh              256          100                 0.1   \n",
       "5             tanh              256           50                 0.1   \n",
       "4             tanh              128          200                 0.1   \n",
       "\n",
       "   mean_test_score  \n",
       "9         0.923872  \n",
       "0         0.917920  \n",
       "8         0.917920  \n",
       "1         0.906328  \n",
       "6         0.900167  \n",
       "3         0.882728  \n",
       "7         0.724937  \n",
       "2         0.629699  \n",
       "5         0.629699  \n",
       "4         0.518588  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameter exploration results\n",
    "param_columns = ['param_' + name for name in params.keys()]\n",
    "pd.DataFrame(search.cv_results_).sort_values('rank_test_score')[param_columns + ['mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb4efccc-04b3-46c4-95dc-b5657777d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9373433589935303"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "search.score(cell_characteristics_test, diagnosis_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
